{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Use watsonx Granite Model Series, Chroma, and LangChain to answer questions (RAG)"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "#### Disclaimers\n\n- Use only Projects and Spaces that are available in watsonx context.\n\n## Notebook content\nThis notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n### About Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires 3 steps:\n\n- Index knowledge base passages (once)\n- Retrieve relevant passage(s) from knowledge base (for every user query)\n- Generate a response by feeding retrieved passage into a large language model (for every user query)\n\n## Contents\n\nThis notebook contains the following parts:\n\n- [Setup](#setup)\n- [Document data loading](#data)\n- [Build up knowledge base](#build_base)\n- [Foundation Models on watsonx](#models)\n- [Generate a retrieval-augmented response to a question](#predict)\n- [Summary and next steps](#summary)\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n##  Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-service-instance.html?context=analytics\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Install and import the dependecies"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "!pip install \"langchain==0.1.7\" | tail -n 1\n!pip install langchain_ibm | tail -n 1\n!pip install wget | tail -n 1\n!pip install sentence-transformers | tail -n 1\n!pip install \"chromadb==0.3.26\" | tail -n 1\n!pip install \"ibm-watsonx-ai>=0.1.7\" | tail -n 1\n!pip install \"pydantic==1.10.0\" | tail -n 1\n!pip install \"sqlalchemy==2.0.1\" | tail -n 1", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Successfully installed annotated-types-0.6.0 anyio-4.3.0 dataclasses-json-0.6.4 exceptiongroup-1.2.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.7 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.21.1 packaging-23.2 pydantic-2.6.3 pydantic-core-2.16.3 sniffio-1.3.1 tenacity-8.2.3 typing-extensions-4.10.0 typing-inspect-0.9.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.1.7 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.19 which is incompatible.\nlangchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.19 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ibm-watson-machine-learning-1.0.349 ibm-watsonx-ai-0.2.0 langchain-core-0.1.29 langchain_ibm-0.1.1 langsmith-0.1.19 orjson-3.9.15\nSuccessfully installed wget-3.2\nSuccessfully installed fsspec-2024.2.0 huggingface-hub-0.21.3 regex-2023.12.25 safetensors-0.4.2 sentence-transformers-2.5.1 tokenizers-0.15.2 transformers-4.38.2\nSuccessfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.7.1 coloredlogs-15.0.1 duckdb-0.10.0 fastapi-0.110.0 h11-0.14.0 hnswlib-0.8.0 httptools-0.6.1 humanfriendly-10.0 lz4-4.3.3 monotonic-1.6 onnxruntime-1.17.1 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 python-dotenv-1.0.1 starlette-0.36.3 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 zstandard-0.22.0\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages (from lomond->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai>=0.1.7) (1.16.0)\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.1.7 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.19 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-1.10.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.1.7 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.19 which is incompatible.\nlangchain-community 0.0.20 requires langsmith<0.1,>=0.0.83, but you have langsmith 0.1.19 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed sqlalchemy-2.0.1\n", "name": "stdout"}]}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "import os, getpass", "execution_count": 2, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.\n\n**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}", "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Please enter your WML api key (hit enter): \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the project id\nThe API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n\n**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "try:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"data\"></a>\n## Document data loading\n\nDownload the file with State of the Union."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "import wget\n\nfilename = 'state_of_the_union.txt'\nurl = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)", "execution_count": 6, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"build_base\"></a>\n## Build up knowledge base\n\nThe most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n\nIn this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\nloader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)", "execution_count": 7, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Create an embedding function\n\nNote that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from langchain.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c577582471324ecd869713194a33d479"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "28392ce371ee46869797c7688a849f5b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e8919d4fee6843b9ae0b22e2de53a02b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "04b7886e3e744e74b237bef2485993e2"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "287f3b107f4741359d5265dfbf5b884c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ec0308de3f7b4c36844061a4c74956f5"}}, "metadata": {}}, {"output_type": "stream", "text": "/opt/conda/envs/Python-RT23.1/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n", "name": "stderr"}, {"output_type": "display_data", "data": {"text/plain": "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "416b21133c4b4660aec55b41729e1ae9"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2b62c88c27774719a8f06c664a85f78d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4491137ec08f4dcc91e6f58823e75685"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "43fe55eb660f4d228844d4eb5cbb129e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a03d3d7651e34eb39d7643b40c037d52"}}, "metadata": {}}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"models\"></a>\n## Foundation Models on `watsonx.ai`"}, {"metadata": {}, "cell_type": "markdown", "source": "IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining model\nYou need to specify `model_id` that will be used for inferencing:"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n\nmodel_id = ModelTypes.GRANITE_13B_CHAT_V2", "execution_count": 9, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the model parameters\nWe need to provide a set of model parameters that will influence the result:"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n}", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### LangChain CustomLLM wrapper for watsonx model\nInitialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`. "}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from langchain_ibm import WatsonxLLM\n\nwatsonx_granite = WatsonxLLM(\n    model_id=model_id.value,\n    url=credentials.get(\"url\"),\n    apikey=credentials.get(\"apikey\"),\n    project_id=project_id,\n    params=parameters\n)", "execution_count": 11, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"predict\"></a>\n## Generate a retrieval-augmented response to a question"}, {"metadata": {}, "cell_type": "markdown", "source": "Build the `RetrievalQA` (question answering chain) to automate the RAG task."}, {"metadata": {}, "cell_type": "code", "source": "from langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())", "execution_count": 12, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Select questions\n\nGet questions from the previously loaded test dataset."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "query = \"What did the president say about Ketanji Brown Jackson\"\nqa.invoke(query)", "execution_count": 14, "outputs": [{"output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "{'query': 'What did the president say about Ketanji Brown Jackson',\n 'result': \" The president said that Ketanji Brown Jackson is one of our nation's top legal minds and will continue Justice Breyer's legacy of excellence.\"}"}, "metadata": {}}]}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "---"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## Summary and next steps\n\n You successfully completed this notebook!.\n \n You learned how to answer question using RAG using watsonx and LangChain.\n \nCheck out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Copyright \u00a9 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}